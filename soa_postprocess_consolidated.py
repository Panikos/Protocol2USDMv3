import json
import sys
from copy import deepcopy

# Load entity mapping for required fields and value sets
def load_entity_mapping(mapping_path="soa_entity_mapping.json"):
    with open(mapping_path, "r", encoding="utf-8") as f:
        return json.load(f)

ENTITY_MAP = None
try:
    ENTITY_MAP = load_entity_mapping()
except Exception:
    ENTITY_MAP = None
    print("[WARNING] Could not load soa_entity_mapping.json. Post-processing will not fill missing fields.")

def make_hashable(o):
    """
    Recursively converts a dictionary or list to a hashable representation
    (tuples of tuples).
    """
    if isinstance(o, (tuple, list)):
        return tuple((make_hashable(e) for e in o))
    if isinstance(o, dict):
        return tuple(sorted((k, make_hashable(v)) for k, v in o.items()))
    if isinstance(o, (set, frozenset)):
        return tuple(sorted(make_hashable(e) for e in o))
    return o

def consolidate_and_fix_soa(input_path, output_path, ref_metadata_path=None):
    """
    Consolidate, normalize, and fully expand a loosely structured SoA file into strict USDM v4.0 Wrapper-Input format.
    Optionally merge in richer metadata from a hand-curated reference file.
    - Enforces top-level keys: study, usdmVersion, systemName, systemVersion
    - Normalizes field names and expands group-based activityTimepoints
    - Preserves and merges metadata (description, code, window, etc.) where possible
    - Validates all references and schema compliance
    - Extracts footnotes, legend, and milestone and outputs a secondary M11-table-aligned JSON for Streamlit
    """
    fixes = []
    # Load files
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    ref_metadata = None
    if ref_metadata_path:
        with open(ref_metadata_path, 'r', encoding='utf-8') as f:
            ref_metadata = json.load(f)

    # --- USDM v4 Wrapper-Input enforcement ---
    required_keys = {'study', 'usdmVersion', 'systemName', 'systemVersion'}
    if not (isinstance(data, dict) and required_keys.issubset(set(data.keys()))):
        print("[INFO] Input missing USDM Wrapper-Input keys. Attempting to fix.")
        # If the input is just the study object itself (no 'study' key at top level)
        if 'study' not in data:
            print("[INFO] Assuming input is the study object. Wrapping.")
            data = {
                'study': data,
                'usdmVersion': '4.0',
                'systemName': 'Protocol2USDMv3',
                'systemVersion': '1.0'
            }
        # If the input has a 'study' key but is missing other wrapper keys
        else:
            print("[INFO] Found 'study' key. Adding missing wrapper keys.")
            data.setdefault('usdmVersion', '4.0')
            data.setdefault('systemName', 'Protocol2USDMv3')
            data.setdefault('systemVersion', '1.0')

    study = data.get('study')
    if not study:
        print("[FATAL] Could not find 'study' object in the input data.")
        sys.exit(1)

    # --- Inject required Study fields to pass schema validation ---
    study.setdefault('name', 'Auto-generated Study Name')
    study.setdefault('instanceType', 'Study')

    # Robustly drill to timeline, handling old and new grouping structures
    versions = study.get('versions') or study.get('studyVersions')
    # Accept single version as dict
    if versions and isinstance(versions, dict):
        versions = [versions]
        study['versions'] = versions
    # If missing or empty, but study itself looks like a version (has timeline), wrap it
    if (not versions or not isinstance(versions, list) or len(versions) == 0):
        # Accept timeline at study, studyDesign, or direct timeline keys
        timeline_candidate = study.get('timeline') or study.get('studyDesign', {}).get('timeline') or study.get('Timeline')
        if timeline_candidate:
            versions = [dict(study)]
            versions[0]['timeline'] = timeline_candidate
            study['versions'] = versions
            print("[INFO] Study missing versions/studyVersions; treating study as a single version.")
        else:
            print("[FATAL] Study must contain a non-empty 'versions' or 'studyVersions' array, and no timeline found in study.")
            print("[DEBUG] Study keys:", list(study.keys()))
            sys.exit(1)
    # --- Inject required StudyVersion fields to pass schema validation ---
    for version in versions:
        version.setdefault('id', 'autogen-version-id-1')
        version.setdefault('versionIdentifier', '1.0.0')
        version.setdefault('rationale', 'Version auto-generated by pipeline.')
        version.setdefault('studyIdentifiers', [])
        version.setdefault('titles', [])
        version.setdefault('instanceType', 'StudyVersion')

    # Accept timeline in various possible locations/names
    timeline = (
        versions[0].get('timeline') or
        versions[0].get('Timeline') or
        versions[0].get('studyDesign', {}).get('timeline') or
        versions[0].get('studyDesign', {}).get('Timeline')
    )
    if not timeline:
        print("[FATAL] No timeline found in study version.")
        print("[DEBUG] Version keys:", list(versions[0].keys()))
        sys.exit(1)
    # Accept timeline as a list (legacy), wrap as dict
    if isinstance(timeline, list):
        timeline = {'plannedTimepoints': timeline}
        versions[0]['timeline'] = timeline
        print("[INFO] Timeline was a list, wrapped as dict.")

    # --- Normalize plannedTimepoints and add milestone support ---
    norm_timepoints = []
    seen_timepoints = set()
    pt_map = {}
    for pt in timeline.get('plannedTimepoints', [],):
        pt_tuple = make_hashable(pt)
        if pt_tuple not in seen_timepoints:
            # Normalize ID fields
            pt_id = pt.get('plannedTimepointId') or pt.get('id')
            if not pt_id:
                print(f"[WARNING] Skipping timepoint with no ID: {pt}")
                continue
            pt['id'] = pt_id
            pt['plannedTimepointId'] = pt_id

            # Merge in metadata from reference if available
            if ref_metadata:
                ref_pts = ref_metadata.get('plannedTimepoints', [])
                ref = next((x for x in ref_pts if (x.get('plannedTimepointId') or x.get('id')) == pt_id), None)
                if ref:
                    for k in ['description', 'code', 'window']:
                        if k in ref:
                            pt[k] = ref[k]
            
            norm_timepoints.append(pt)
            pt_map[pt_id] = pt
            seen_timepoints.add(pt_tuple)
    timeline['plannedTimepoints'] = norm_timepoints

    # --- Normalize activities ---
    act_map = {}
    norm_acts = []
    acts = timeline.get('activities', [])
    if acts and isinstance(acts[0], str):
        for i, act_str in enumerate(acts):
            act = {"activityId": f"A{i+1}", "activityName": act_str}
            norm_acts.append(act)
            act_map[act['activityId']] = act
        fixes.append("Converted activities from strings to objects.")
    else:
        for act in acts:
            act = deepcopy(act)
            act_id = act.get('activityId') or act.get('id')
            if act_id:
                act['activityId'] = act_id
            if 'name' in act and 'activityName' not in act:
                act['activityName'] = act['name']
            norm_acts.append(act)
            act_map[act['activityId']] = act
    timeline['activities'] = norm_acts

    # --- Normalize activityGroups ---
    # First, map all existing groups by their ID for easy lookup.
    group_map = {g.get('activityGroupId'): g for g in timeline.get('activityGroups', []) if g.get('activityGroupId')}
    # For now, we are not performing group inference, so the final list of groups is just the ones from the input.
    norm_groups = list(group_map.values())
    timeline['activityGroups'] = norm_groups

    # --- Process ActivityTimepoints ---
    atps = timeline.get('activityTimepoints', [])
    new_atps = []
    dropped = []
    
    # Handle multiple formats in a single pass, robustly handling nested ID objects
    for atp in atps:
        try:
            # Format 1: Direct IDs (most common from vision)
            if 'activityId' in atp and 'plannedTimepointId' in atp:
                act_id = atp['activityId']
                if isinstance(act_id, dict): act_id = act_id.get('id')

                pt_id = atp['plannedTimepointId']
                if isinstance(pt_id, dict): pt_id = pt_id.get('id')

                if act_id and pt_id and act_id in act_map and pt_id in pt_map:
                    new_atps.append({'activityId': act_id, 'plannedTimepointId': pt_id})
                else:
                    dropped.append({**atp, 'reason': 'invalid activityId or plannedTimepointId'})
            
            # Format 2: Group-based
            elif 'activityGroupId' in atp and 'plannedTimepointId' in atp:
                group = next((g for g in norm_groups if g.get('id') == atp['activityGroupId']), None)
                pt_id = atp['plannedTimepointId']
                if isinstance(pt_id, dict): pt_id = pt_id.get('id')

                if group and pt_id and pt_id in pt_map:
                    for act_id_from_group in group.get('activityIds', []):
                        if act_id_from_group in act_map:
                            new_atps.append({'activityId': act_id_from_group, 'plannedTimepointId': pt_id})
                        else:
                            dropped.append({'activityId': act_id_from_group, 'plannedTimepointId': pt_id, 'reason': 'invalid activityId in group expansion'})
                else:
                    dropped.append({**atp, 'reason': 'group not found or invalid timepointId'})
            
            # Format 3: Activity-based with a list of timepoints
            elif 'activityId' in atp and 'plannedTimepointIds' in atp:
                act_id = atp['activityId']
                if isinstance(act_id, dict): act_id = act_id.get('id')

                if act_id and act_id in act_map:
                    for ptid_item in atp['plannedTimepointIds']:
                        ptid = ptid_item
                        if isinstance(ptid, dict): ptid = ptid.get('id')

                        if ptid and ptid in pt_map:
                            new_atps.append({'activityId': act_id, 'plannedTimepointId': ptid})
                        else:
                            dropped.append({'activityId': act_id, 'plannedTimepointId': ptid, 'reason': 'invalid plannedTimepointId in list'})
                else:
                    dropped.append({**atp, 'reason': 'invalid activityId'})
            
            # Unrecognized format
            else:
                dropped.append({**atp, 'reason': 'unrecognized format'})
        except Exception as e:
            dropped.append({**atp, 'reason': f'processing error: {e}'})

    # --- Fallback: If no activityTimepoints, try to infer from activities ---
    if not new_atps:
        for act in norm_acts:
            aid = act.get('activityId') or act.get('id')
            for k in ['plannedTimepoints', 'plannedTimepointIds']:
                if k in act:
                    for ptid in act[k]:
                        if aid and ptid:
                            new_atps.append({'activityId': aid, 'plannedTimepointId': ptid})
        if new_atps:
            fixes.append('Auto-generated activityTimepoints from per-activity plannedTimepoints.')

    timeline['activityTimepoints'] = new_atps

    # --- Fill missing fields using entity mapping ---
    def fill_missing_fields(entity_type, obj):
        if not ENTITY_MAP or entity_type not in ENTITY_MAP:
            return
        mapping = ENTITY_MAP[entity_type]
        for field, meta in mapping.items():
            if field not in obj:
                # Use empty string or placeholder for missing required fields
                if 'allowed_values' in meta:
                    obj[field] = meta['allowed_values'][0]['term'] if meta['allowed_values'] else ''
                else:
                    obj[field] = ''
            # Normalize coded values
            if 'allowed_values' in meta and obj[field]:
                allowed = [v['term'] for v in meta['allowed_values']]
                if isinstance(obj[field], list):
                    obj[field] = [v if v in allowed else allowed[0] for v in obj[field]]
                else:
                    if obj[field] not in allowed:
                        obj[field] = allowed[0]
    # Study
    fill_missing_fields("Study", data)
    for sv in data.get("studyVersions", []):
        fill_missing_fields("StudyVersion", sv)
        sd = sv.get("studyDesign", {})
        timeline = sd.get("timeline", {})
        fill_missing_fields("Timeline", timeline)
        # PlannedTimepoints
        pt_map = {}
        unhandled_timepoints = []
        for pt in timeline.get('plannedTimepoints', []):
            # Accept both 'plannedTimepointId' and 'plannedVisitId' as equivalent
            pt_id = pt.get('plannedTimepointId') or pt.get('plannedVisitId')
            if pt_id is not None:
                pt['plannedTimepointId'] = pt_id  # Normalize key
                pt_map[pt_id] = pt
            else:
                print(f"[WARNING] Skipping timepoint missing both plannedTimepointId and plannedVisitId: {pt}")
                unhandled_timepoints.append(pt)
        if unhandled_timepoints:
            print(f"[SUMMARY] {len(unhandled_timepoints)} timepoints skipped due to missing IDs.")
        # Activities
        for act in timeline.get("activities", []):
            fill_missing_fields("Activity", act)
        # ActivityGroups
        for ag in timeline.get("activityGroups", []):
            fill_missing_fields("ActivityGroup", ag)
        # ActivityTimepoints
        for atp in timeline.get("activityTimepoints", []):
            fill_missing_fields("ActivityTimepoint", atp)
    # --- Save and report ---
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"[CONSOLIDATE/FIX] {len(new_atps)} valid activityTimepoints. {len(dropped)} dropped. {len(norm_timepoints)} timepoints, {len(norm_acts)} activities, {len(norm_groups)} groups.")
    if fixes:
        print("Fixes applied:")
        for fix in fixes:
            print("- ", fix)
    if dropped:
        print("First 10 dropped:")
        for d in dropped[:10]:
            print(d)
    else:
        print("No invalid links found.")

if __name__ == "__main__":
    if len(sys.argv) not in [3, 4]:
        print("Usage: python soa_postprocess_consolidated.py <input.json> <output.json> [reference_metadata.json]")
        sys.exit(1)
    consolidate_and_fix_soa(*sys.argv[1:])
